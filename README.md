# FastCodeR1

Locally Hosted Coding Assistant!

Just install the extension (& ensure the Requirements are satisfies) and run the `Chat with DeepSeek!` command in your command palette.

## Features

- Talk with the State of the Art DeepSeek-R1 model locally
- Have complete conversations with user chat history support
- Get fast help with your projects

## Requirements

Ensure ollama is installed with the 1.5b model installed.

Run the following command in powershell after installing ollama to ensure the weights are downloaded.
```bash
ollama run deepseek-r1:1.5b  
```

## Release Notes

### 0.0.1
This is the prerelease, it implements the idea with a basic UI.
The current version supports:
- Ability to chat locally in VSCode Environment
- Direct support for 1.5b QWEN distill

The next version will have the following features:
- Ability to Change Model
- Code Block Rendering
- Think Tags Rendering
- User Chat History

---

**Enjoy!**